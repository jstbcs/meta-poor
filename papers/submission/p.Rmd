---
title             : "Does Every Study? Implementing Ordinal Constraint in Meta-Analysis"
shorttitle        : "Ordinal Constraint in Meta-Analysis"

author: 
  - name          : "Julia M. Haaf"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Postbus 15906, 1001 NK AMSTERDAM, The Netherlands"
    email         : "j.m.haaf@uva.nl"
  - name          : "Jeffrey N. Rouder"
    affiliation   : "2"

affiliation:
  - id            : "1"
    institution   : "University of Amsterdam"
  - id            : "2"
    institution   : "University of California-Irvine"

authornote: |
  Julia M. Haaf, Psychological Methods Unit, University of Amsterdam, Amsterdam, Netherlands; Jeffrey N. Rouder, Department of Cognitive Sciences, University of California, Irvine, USA.

  We are indebted to Sho Tsuji and Julia Carbajal for letting us use their familiar-word-recognition meta-analysis and for helping us make sense of the interpretation of the findings. We thank Paul Speckman and Quentin Gronau for insightful discussions about variable transformations and meta-analysis. This manuscript and analysis code are available at https://github.com/jstbcs/meta-poor.

abstract: |
  The most prominent goal when conducting a meta-analysis is to estimate the true effect size across a set of studies. This approach is problematic whenever the analyzed studies are inconsistent, i.e. some studies show an effect in the predicted direction while others show no effect and still others show an effect in the opposite direction. In case of such an inconsistency, the average effect may be a product of a mixture of mechanisms. The first question in any meta-analysis should therefore be whether all studies show an effect in the same direction. To tackle this question a model with multiple ordinal constraints is proposed---one constraint for each study in the set. This "every study" model is compared to a set of alternative models, such as an unconstrained model that predicts effects in both directions. If the ordinal constraints hold, one underlying mechanism may suffice to explain the results from all studies. A major implication is then that average effects become interpretable. We illustrate the model-comparison approach using Carbajal et al.'s (2020) meta-analysis on the familiar-word-recognition effect, show how predictor analyses can be incorporated in the approach, and provide R-code for interested researchers. As common in meta-analysis, only surface statistics (such as effect size and sample size) are provided from each study, and the modeling approach can be adapted to suit these conditions. 
  
keywords          : "meta-analysis; Bayesian inference; order-constrained inference"
wordcount         : "7604 words in text body."

bibliography      : ["lab.bib", "r-references.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

header-includes:
  - \usepackage{mathrsfs}
  - \usepackage[makeroom]{cancel}
  - \usepackage{pcl}
  - \usepackage{setspace}\doublespacing
  - \usepackage{marginnote}
  - \newcommand{\readme}[1]{\emph{\marginnote{Julia} (#1)}}
  - \usepackage{pifont}
  - \usepackage{hyperref}
  - \usepackage{colortbl}
  - \hypersetup{colorlinks=true,urlcolor=blue,citecolor=black,linkcolor=black}

class             : "man"
output            : papaja::apa6_pdf
csl               : apa6.csl
---

```{r setup, include=FALSE, echo = F}
set.seed(666)

# knitr::opts_chunk$set(echo = TRUE)

library('msm')
library("MCMCpack")
library("psych")
library("papaja")
library("metafor")
library("ggplot2")
library("truncdist")
library("spatialfil")
library("tmvtnorm")
library("mvtnorm")
library("scam")

theme_set(theme_apa(base_size = 10))
# Hack to overcome ggplot2 bug (https://github.com/tidyverse/ggplot2/issues/2058)
assignInNamespace("theme_nothing", function() { theme_void() + theme(axis.text.x = NULL, axis.text.y = NULL, axis.line.x = element_blank(), axis.line.y = element_blank()) }, "cowplot")
```

In meta-analysis it is common to estimate a meta-analytic mean across a corpus of studies, as well as to assess how this mean depends on critical covariates. Consider the question of whether infants can distinguish familiar words from novel words [@Carbajal:etal:2020]. Because the data are relatively expensive in infant research, meta-analysis is an important tool to address this question. The meta-analytic mean difference may be estimated between the critical conditions of familiar and novel words, and assessed for the typical dependent variables such as looking times or head turns. Moreover, the variable age serves as a critical covariate for theoretical development and guidance of future studies.

Although the meta-analytic mean difference may seem natural and uncontroversial, we have previously questioned its appropriateness in meta-analytic studies [@Rouder:etal:2019b]. We started with the observation that for many experimental paradigms effects that are positive, zero, or negative correspond to different psychological phenomena. For the familiar-word-recognition effect the three relations---negative, null, positive---correspond to a novelty effect, an invariance, and a habituation effect, respectively. In fact, these three relations may correspond to different stages in early language development [@Halle:Boysson:1994]. During the first few months babies do not distinguish familiar words and rare or novel words, and studies investigating the familiar-word-recognition effect with up to 11 month old infants may expect a zero effect. During the next stage of language development, infants may pay more attention to familiar words than novel ones resulting in an expected positive familiar-word-recognition effect. After around 20 months processing of words changes again, and infants may even pay more attention to novel words [@Halle:Boysson:1994]. This distinction between negative, zero, and positive effects poses a problem for conventional meta-analysis. Consider two studies, both ostensibly run on the same population that had true effect sizes of 0.4 in one study, and of -0.2 in another. How well does the average of 0.1 describe either of the two studies? Can we conclude that there is a small positive familiar-word-recognition effect where babies pay more attention to familiar words than novel words?

Motivated by the concern that qualitatively different outcomes correspond to different psychological processes, @Rouder:etal:2019b recommend assessing the following question: How plausible is it that all true study effects are qualitatively the same, that is, all effects are in the same direction, or that some effects are qualitatively different, that is, they are in opposite directions? The distinction between true and observed effects is critical here. A true effect is a study's underlying latent effect, and if the study had infinitely many observations we could observe its true effect. In reality though, we only observe a limited number of observations, and this limited knowledge introduces sample noise. The observed effect is therefore a combination of the true effect and sample noise. Sample noise increases the variability of a collection of observed effects in that observed effects are more variable than true effects. To separate sample noise from true variability, @Rouder:etal:2019b used hierarchical modeling. This approach separates true variability from sample noise, and allows to answer questions about the collection of true effects.

If all true effects are plausibly in the same direction, then the average across these effects is much more interpretable as an overall effect of a common phenomenon. Conversely, if true effects are in opposite directions, the average in uninterpretable, and the data pattern begs for the discovery of critical covariates that explain why some effects are positive and some are negative. This issue is discussed in the clinical literature as *quantitative* vs. *qualitative interactions* [@Gail:Simon:1985; @Pan:Wolfe:1997]. In clinical settings, qualitative interaction refers to the case that one treatment is superior for one patient population, and another treatment is superior for another patient population. As such, the effect between the two treatment conditions is opposite for some patients. In this case, recommendation for treatment has to be qualified by the target population. @Gail:Simon:1985 proposed a frequentist likelihood ratio test for qualitative interactions for different treatments across patients. In this test, the null hypothesis is that a treatment is effective for all patient populations. Therefore, if we fail to reject the null hypothesis then this is interpreted as evidence for quantitative rather than qualitative differences. @Pan:Wolfe:1997 further developed the test to assess qualitative differences across studies rather than patients. Here the focus is on different patient subpopulations instead of different patients within a study. @Higgins:etal:2009 use Bayesian estimation methods to assess qualitative interactions. They assess the 95% posterior prediction interval for a new study's true effect. If the interval includes both positive and negative values then they interpret this outcome as evidence for qualitative interactions. Even though @Higgins:etal:2009 developed this test they note significant limitations of it. What is needed is a proper Bayesian test such as Bayes factor model comparison [@Higgins:etal:2009; p. 147]. In light of Higgins and colleagues' self critique, one of the main contributions of @Rouder:etal:2019b is therefore a Bayes factor approach of determining whether a set of studies plausibly share a common phenomenology. They call this the *does-every-study meta-analysis*. 

There are three obvious constrained models to consider: A *positive-effects model* where all true study effects are constrained to be positive, a *negative-effects model* where all true study effects are constrained to be negative, and a *null model* where all study effects are constrained to be exactly zero. The alternative to these constrained models is a mixture where some studies are negative, positive, and null. @Rouder:etal:2019b called this mixture the *unconstrained model*. Knowing whether the unconstrained model holds or whether one of the constrained models hold is critical for understanding any psychological phenomenon, and to determine the required theoretical complexity. The four models will be developed subsequently.

There are a number of difficulties in the approach proposed by @Rouder:etal:2019b. First, the raw, participant-level data from each study are needed to fit the hierarchical model. While there are some examples of collections of studies where raw data are available [e.g. @Ebersole:etal:2016; @Corker:etal:2017; @Wagenmakers:etal:2016b], the vast majority of meta-analyses is based on published summary statistics rather than raw data. Second, @Rouder:etal:2019b require the same dependent measures across all studies. In many meta-analyses the dependent measures and designs vary across studies. For example, dependent variables may be accuracy in some studies, and response times in others. It is unreasonable to assume that all studies in a corpus assess the same items in the same questionnaires. Third, the methods by @Rouder:etal:2019b  is not capable to assess effects of study-level predictor variables. In sum, the model comparison approaches that we previously advocated were developed for many-labs type replications where there is a great deal of uniformity, control, and openness in assessing a single question. It was simply not developed for common meta-analyses where designs and scales vary, and where data extraction relies on published information.

Here, we develop the does-every-study approach for common meta-analysis. We address all three of these limitations by developing models on extant summary statistics. In contrast to @Rouder:etal:2019b, the current development is widely applicable.

In the next section, we present the collection of formal does-every-study models that we subsequently analyze. Following that, we develop meta-regression approaches to understand whether critical covariates affect the underlying phenomenology. We apply the does-every-study analysis to meta-analytic data on the familiar-word-recognition effect reported by @Carbajal:etal:2020. We discuss the importance of prior settings for Bayesian meta-analysis, limitations of the methods, and future directions of does-every-study modeling.

# Does-Every-Study Models

The first step of analysis is to specify what constitutes the data. This choice is limited by the information available in published articles. We decided to use the Fisher's $Z$ effect-size statistic which is typically used to assess the size of a correlation coefficient. Fisher's $Z$ is easily derived from other popular effect-size statistics such as Cohen's $d$ or log odds ratio. Fisher's $Z$ is a variance stabilizing transformation of the bivariate correlation coefficient $r$ that maps from the (-1, 1) interval into the real number space:

\begin{align}\label{eq:Zcalc}
Z = \frac{1}{2} \, \log \left(\frac{1 + r}{1 - r}\right) = \mbox{arctanh}(r).
\end{align}

For those unfamiliar with the statistic Fisher's $Z$, the relationship to the correlation coefficient is a sigmoidal function that is $Z = -\infty$ for $r = -1$, $Z = 0$ for $r = 0$, and $Z = \infty$ for $r = 1$. In the large-sample limit, the distribution of $Z$ is known:

\begin{align}\label{eq:fishersZ}
Z \sim \mbox{Normal} \left(\theta, \frac{1}{N - 3}\right),
\end{align}

where $\theta$ is the true effect size and $N$ is the number of observations. Importantly, the variance of $Z$ is only dependent on $N$, and is not affected by the size of the correlation. This property makes Fisher's $Z$ an attractive target for modeling. Equation 2 is typically derived for bivariate correlations of two continuous variables. Yet, we also wish to use the approach for experimental settings where one of the variables is discrete. To our knowledge, Equation 2 has not been derived this case, that is, for biserial correlations. Indeed it does, and we provide the proof in the appendix.

To address qualitative sameness and differences, we start out the same way as common meta-analysis. Models are placed on the collection of study effects much like in what is often called random-effects meta-analysis. For the following models, let $i$ denote the study, $i = 1, \ldots, I$. The base model here is 

\begin{equation}\label{eq:basemodel}
Z_i \sim \mbox{Normal} \left(\theta_i, \frac{1}{N_i - 3}\right),
\end{equation}

where $\theta_i$ is the $i$th study's true $Z$-value. We may place models with ordinal and equality constraints on the collection of these true study effects $\theta_i$. 

### The Unconstrained Model

The unconstrained model is a standard linear model without constraints, and it corresponds to the typical random-effects meta-analytic model. Here, the collection of study effects simply follows a normal distribution:

\[
\begin{array}{llr}
\calM_u: && \theta_i \stackrel{iid}{\sim} \mbox{Normal}(\mu,\tau^2),\\
\end{array}
\]

where $\mu$ is the mean effect and $\tau^2$ is the variance of effects. No constraints are placed on the collection of $\theta_i$ such that effects for some studies may truly be positive while effects for other studies may truly be negative. 

### The Positive-Effects Model

The positive-effects model corresponds to the hypothesis that every study has a true effect in the same, expected direction:

\[
\begin{array}{llr}
\calM_p: && \theta_i \stackrel{iid}{\sim} \mbox{Normal}_+(\mu,\tau^2),\\
\end{array}
\]

where $\mbox{Normal}_+$ is a normal distribution truncated below at zero. The positive domain of the model translates to the idea that effects are coded so that positive is the expected direction. This model statement implies multiple ordinal constraints, one for each study in the data set, reducing model complexity drastically compared to the unconstrained model. Assuming the same mean and variance as the general model, the positive-effects model has higher density for (small) positive effects and no mass below zero.

### The Common-Effect Model

The common-effect model corresponds to the frequentist fixed-effect meta-analytic model [@Borenstein:etal:2010]. For the common-effect model, all $\theta_i$ have the same value:

\[
  \begin{array}{llr}
\calM_c: && \theta_i = \mu, \quad \mu > 0.\\
\end{array}
\]

Here, the common effect, $\mu$, is restricted to be in the expected direction, just like in the positive-effects model.

The common-effect model may seem unlikely, and there has been much philosophical and statistical debate about the usefulness of meta-analytic common-effect models [e.g. @Hedges:Vevea:1998]. Surely, there must be some variability between true effects across studies. Yet, especially in meta-analysis, this model may be a necessary addition to the set of models proposed. @Borenstein:etal:2010 note, for example, that between study variability may be hard to estimate with small numbers of studies in a data set. Also, the common effect assumption may be meaningful when the experiments where pre-planned to use the exact same methods across labs. This pre-planning is common in medical research, and it recently gained attention in psychological research with the many-labs projects [@Ebersole:etal:2016]. In fact, @Rouder:etal:2019b report evidence for this model for Ebersole and colleagues' many-labs study on moral credentialism.

### The Null Model

The last model proposed is a null model. Here, all $\theta_i$ are exactly zero:

\[
  \begin{array}{llr}
\calM_0: && \theta_i = 0.\\
\end{array}
\]

This model is the most constrained model of the set, and it is a strict null in that it does not allow for true variability around zero. Instead, all variation must be captured in the noise term $\frac{1}{N_i - 3}$. If an effect truly does not exist, then all studies truly should have a null result. True variation around zero would imply that there has to be a trade-off between the studies such that all true effects sum to zero, which strikes us as highly unlikely.

### Prior Settings

```{r child="analysis/transformations.Rmd"}
rerun <- 1
```

For Bayesian analysis, priors are needed on the mean and variance parameters. Here, prior distributions are placed on $\mu$, the true mean effect size, and on $\tau^2$, the true variability of effect sizes. We chose conjugate prior distributions:

\begin{align*}
\mu &\sim \mbox{Normal}(0, c_1^2),\\
\tau^2 &\sim \mbox{Inverse-Gamma}(1, c_2^2).
\end{align*}

Here, we set $c_1 = c_2 = 0.15$. These settings can be interpreted as scale settings on the overall effect ($c_1$) and the between-study heterogeneity ($c_2$). Figure\ \@ref(fig:fig-transform)A and B show the resulting prior distributions on $\mu$ and $\tau^2$. These priors are critical for model comparison because they differ between the models. For example, the common-effect model constrains $\tau^2$ to zero. Therefore, the prior settings have to be chosen with some care. One way of understanding these priors is to see their consequence on individual-study Fisher-z statistics.  The priors on $\mu$ and $\tau^2$ may be integrated to form a *marginal prior* on $\theta_i$, and this marginal is shown in Figure\ \@ref(fig:fig-transform)C. We chose the priors on $\mu$ and $\tau$ so that values of $\theta_i$ between -0.5 and 0.5 are most plausible under the unconstrained model. This prior corresponds to a relatively informative distribution while still allowing for the possibility of untypically large effect sizes.  We believe that the current settings are appropriate for many applications in psychology. Nevertheless, a sensitivity analysis for varying prior settings is crucial to assess whether the variability of Bayes factor model comparison results are reasonable. We provide more discussion on the issue and a sensitivity analysis subsequently.

### Priors on Other Effect Size Measures

Typically, prior settings should be made by researchers familiar with the scaling of the dependent variable [@Haaf:Rouder:2017; @Rouder:etal:2018]. Here, the random variable is effect size, and one might think the scaling is obvious. Yet, we note here that different measures of effect size have different scaling, and an intuition for expected ranges of, say Cohen's $d$ may not be helpful when developing models for Fisher's $Z$ (See Figure\ \@ref(fig:fig-transform)D-F). 

To assess our prior choice for different effect size measures, we transformed the marginal prior on Fisher's $Z$ for other popular effect size statistics: Cohen's $d$, Hedges' $g$, and log odds ratio (LOR). Figure\ \@ref(fig:fig-transform) shows these transformed priors on these alternative measures. Importantly, the key property for our analysis, the direction of the effect, is preserved across all transformations. However, the value of the effect size is different across measures. The distributions of Cohen's $d$ and Hedges' $g$ are about twice as wide as the distribution of Fisher's $Z$, and the distribution of log odds ratio is approximately four times as wide as the distribution of Fisher's $Z$.

These transformations imply that recommended default prior settings for Bayesian meta-analysis must be transformed for the different effect sizes measures. There are two possibilities on how to achieve the same transformed prior settings across different effect size measures. First, researchers may transform their effect size statistic to Fisher's $Z$ and use the prior settings proposed here. Or, second, researchers may adjust prior settings of $c_1$ and $c_2$.^[Additionally, note that the variance of the distribution of, say Cohen's $d$ is not $\frac{1}{N - 3}$. For between-subject designs it is $V_d = \frac{n_1 + n_2}{n_1 \times n_2} + \frac{\delta^2}{2(n_1 + n_2 - 2)},$ where $n_1$ and $n_2$ are the sample sizes of the two conditions, and $\delta$ is the true effect size. Therefore, the variance of observed effect sizes in Equation\ \@ref(eq:basemodel) needs to be adjusted as well.] For example, if Cohen's $d$ is used instead of Fisher's $Z$, a quick fix would be to double the scales $c_1$ and $c_2$. 

# Model Comparison

The main purpose of the modeling approach is to draw inference about the distribution of study effects. 
To do so, we propose a Bayes factor model comparison approach to compare the unconstrained model, the positive-effects model, the common-effect model, and the null model. Here, we provide a very brief, informal discussion of Bayes factors and the approaches used to estimate them. We have previously provided more extensive discussions in @Haaf:Rouder:2017 and @Rouder:etal:2019b that were based on @Jeffreys:1961, @Kass:Raftery:1995, @Morey:etal:2016, and @Rouder:etal:2016b.

<!-- Bayes rule for two models, $\calM_1$ and $\calM_2$ is -->

<!-- \begin{equation}\label{bayesrule} -->
<!-- \frac{P(\calM_1 \mid \bfY)}{P(\calM_2 \mid \bfY)} = \frac{P(\bfY \mid \calM_1)}{P(\bfY \mid \calM_2)} \times \frac{P(\calM_1)}{P(\calM_2)}, -->
<!-- \end{equation} -->

<!-- where $\frac{P(\calM_1 \mid \bfY)}{P(\calM_2 \mid \bfY)}$ are the posterior odds, $\frac{P(\calM_1)}{P(\calM_2)}$ are the prior odds, and $\frac{P(\bfY \mid \calM_1)}{P(\bfY \mid \calM_2)}$ is the Bayes factor. The Bayes factor is also referred to as the updating factor because it is the amount by which the prior odds need to be updated in the light of the data to get to the posterior odds.  -->

<!-- The Bayes factor is therefore the evidence for $\calM_1$ relative to $\calM_2$. -->

```{r fig-modelcomp, child="figures/modelcomp.Rmd"}
```

In Bayesian model comparison, the main target of interest is the relative evidence for one model compared to another. The Bayes factor is this relative evidence, and it results directly from Bayes rule. We may also view the Bayes factor as *predictive accuracy* of one model over another model [@Rouder:Morey:2019]. In this sense, the Bayes factor denotes how well the first model predicted the observed data compared to the other model. Figure\ \@ref(fig:meta-modelcomp) illustrates this point. Panel A shows the model specifications for the positive-effects and the unconstrained model for any one study's true effect. The predictions for observed effect sizes from these models are illustrated in panel B. As can be seen, the positive-effects model best predicts positive effects while the general model predicts both positive and negative effects to the same degree. As a result, if a positive effect is observed, the positive-effects model will be preferred as it has more density for positive effects than the general model. In contrast, if a negative effect is observed the general model will be preferred. Yet, the positive-effects model *can* predict small negative effects despite the ordinal constraint on true effects.

<!-- As a side note, it is a major advantage of Bayesian analysis to allow for the calculation of predictions from a model *before* observing any data. In the frequentist setting, predictions are always based on observed data, for example using cross-validation [@Rouder:etal:2016b]. -->

How do the model predictions extend to more than one study effect? Figure\ \@ref(fig:meta-modelcomp) shows multivariate model specifications for two studies for both the unconstrained and the positive-effects model. Panels C and D show model specification and model prediction for the positive-effects model. The effect size for Study 1 is specified on the x-axis; the effect size for Study 2 is specified on the y-axis. The correlation between the two effects is introduced by the hierarchical nature of the models (i.e. the common variability of the mean effect). This correlation is also preserved in the predictions, and the positive-effects model best predicts small, similar, positive effects. The predictions of the unconstrained model are shown in Figure\ \@ref(fig:meta-modelcomp)F. Conceptually, Bayes factors can be understood as a comparison between the prediction in panels D and F in Figure\ \@ref(fig:meta-modelcomp). If we observed effects for the two studies, say $z_1 = .2$ and $z_2 = .25$, then we may compare the predictive accuracy for the point $[.2, .25]$ of these two panels. The ratio of the predictive accuracies at the observed data point is the Bayes factor between the two models. 

Practically, Bayes factors can be estimated in several ways. Here, we use the encompassing approach to estimate the Bayes factor between the unconstrained model and the positive-effects model [@Hoijtink:2012; @Klugkist:etal:2005; @Haaf:Rouder:2017], and an analytic approach to assess the Bayes factor between the unconstrained model, the common-effect model, and the null model [@Rouder:etal:2012]. The Bayes factors between the positive-effects model, the common-effect model, and the null model can be obtained using the transitivity property of Bayes factors.^[Using transitivity, the Bayes factor between the positive-effects model and the null model, $B_{+0}$, may be obtained as $B_{+0} = \frac{B_{g0}}{B_{g+}}$.]

The encompassing approach employed here is based on @Hoijtink:2012, and we have employed and described the approach in @Haaf:Rouder:2017, @Haaf:Rouder:2019, and @Rouder:etal:2019b. More interesting is the analytic approach employed here. Because we use surface statistics (such as Fisher's $Z$) and their standard errors as data, we derived the analytic Bayes factor between the unconstrained model and the common-effect model, and between the unconstrained model and the null model from scratch. The main target of the approach is the probability of data conditional on each of the two models marginalized over the parameter space. This probability is typically called the marginal probability of a model, and it may be expressed using The Law of Total Probability as
\begin{equation} \label{eq:bfInt}
P(\bfY \mid \calM) = \int_{\bfxi \in \Xi} P(\bfY|\bfxi)P(\bfxi)d\bfxi,
\end{equation}
where $\bfxi$ is a vector of parameters from parameter space $\Xi$. The likelihood function, $P(\bfY|\bfxi)$, is given by Equation\ \@ref(eq:basemodel), and it is the product of normal densities with mean $\theta_i$ and variance $\frac{1}{n_i - 3}$ evaluated for the data. 

Here, we illustrate the analytic approach for obtaining the Bayes factor between the null model and the unconstrained model. For the null model, obtaining the marginal probability is straight-forward. The parameter space of $\theta_i$ is reduced to a vector of zeros. The integral in Equation \@ref(eq:bfInt) is simply the likelihood of the data when $\theta_i = 0$ for all $i$. For the unconstrained model, the integral in Equation \@ref(eq:bfInt) may be simplified by integrating out the collection of $\theta_i$ and the overall effect $\mu$. The likelihood of $Z_i$ marginal over $\theta_i$ and $\mu$ is

\[Z_i | \tau^2, \sigma^2_\mu \sim \mbox{Normal}(0, \frac{1}{n_i - 3} + \tau^2 + \sigma^2_\mu),\]

where $\tau^2$ is the variance of $\theta_i$ and $\sigma^2_\mu$, the variance of $\mu$ that is here fixed at $.15^2$. The integral is now reduced to the dimension of $\tau^2$. To estimate $P(\bfY \mid \calM)$ the integral can be  evaluated for possible values of $\tau^2$ based on its prior distribution. The marginal probability can be estimated in similar fashion for the common-effect model. 


```{r, eval = F}
### Unconstrained model
R <- 100000
s2_mu <- rinvgamma(R, 2, .01)
s2_theta <- rinvgamma(R, 2, .01)
get.log.likeli.g <- function(s2, z, n){ #s2 = vector of s2_mu and s2_theta
  sum(dnorm(z, 0, sqrt(1/(n - 3) + s2[1] + s2[2]), log = T))
}

r.likeli <- apply(cbind(s2_mu, s2_theta), 1, get.log.likeli.g, z = zs, n = ni)
Mg <- mean(exp(r.likeli))

test <- apply(cbind(s2_mu, s2_theta), 1, function(s2, n){sqrt(1/(n - 3) + s2[1] + s2[2])}, n = ni[1])

### Null model

lM0 <- sum(dnorm(zs, 0, sqrt(1/(ni - 3)), log = T))

Mg / exp(lM0)
```

# Application: The Familiar-word-recognition Effect

```{r child = "analysis/analysis_carbajal.Rmd", warning=F}
```

```{r}
I <- nrow(dat)
dat$mean_age_months <- dat$mean_age/30
```

We are now ready to apply the does-everyone meta-analysis to data. Here, we re-analyzed the meta-analysis on the familiar-word-recognition-effect conducted by @Carbajal:etal:2020. The authors gathered `r I` studies from `r length(unique(dat$short_cite))` research articles studying infants with average sample age ranging from `r round(range(dat$mean_age_months))[1]` to `r round(range(dat$mean_age_months))[2]` months. Here, null, positive, and negative effects correspond to different stages in early language development [@Halle:Boysson:1994]. If infants do not distinguish between familiar and unfamiliar words then the familiar-word-recognition effect is zero. If infants pay more attention to familiar words than novel ones then the familiar-word-recognition effect is positive. If infants pay more attention to novel words than familiar words then the familiar-word-recognition effect is negative. The expectation is that infants in the age range of the current studies show a positive familiar-word recognition effect. The analysis by @Carbajal:etal:2020 showed an overall effect size in line with this expectation, $\hat{\mu} = `r round(freqb, 2)`$, $95\%$CI $= `r paste0("[", round(freqci[1], 2), ", ", round(freqci[2], 2), "]")`$. Yet, do all studies plausibly show a qualitatively similar effect, that is, is there one underlying phenomenology?

We first estimated the unconstrained model. The observed and estimated study effect sizes (Fisher's $Z$) are shown in Figure\ \@ref(fig:resFig). The gray squares correspond to the observed effect sizes, and the black points correspond to the estimates from the unconstrained model (posterior means). The gray error bars show the 95% confidence intervals for Fisher's $Z$. The black error bars show 95% credible intervals. The size of the points is determined by the study weights that are in turn dependent on the sample size. These weights determine the impact of each study on the overall effect size. As can be seen, there is a substantial amount of hierarchical shrinkage reducing the variability of estimated study effects as compared to observed study effects. The amount of shrinkage for each study is a function of the standard error of the effect size estimate, which in turn is a function of the sample size. Because the studies in the current meta-analysis have relatively small sample sizes---between `r range(dat$n)[1]` and `r range(dat$n)[2]` participants---as is typical for developmental research, much hierarchical shrinkage is expected towards the overall effect.

Figure\ \@ref(fig:resFig) also shows that while three studies have an observed negative effect size the posterior means for all `r I` studies are positive. To quantify the evidence for or against every-study-does, we compare the models previously proposed using Bayes factors. The preferred model is the positive-effects model, and it is preferred over the common-effect model by `r round(1/bfs.cont$bfs[3] * bfs.cont$bfs[4])` to 1, over the null model by `r round(1/bfs.cont$bfs[3] * bfs.cont$bfs[5])` to 1, and over the unconstrained model by `r round(1/bfs.cont$bfs[3])` to 1 (see Table\ \@ref(tab:covariate-tab)). Hence, while there is some evidence that all studies show a positive effect, there is only modest evidence that study effects actually differ in true value.

In summary, at this point, it seems that all studies conform with the expectation that infants pay more attention to novel words. Hence, we may tentatively hold onto the notion that all studies are indeed tapping the same psychological processes and belong functionally to a common phenomenology. We find relatively small differences across estimated true effects. Given the common phenomenology we now feel confident to interpret the overall effect across studies, estimated as $\hat{\mu} \mid \bfZ = `r round(mean(rec.mutheta), 2)`$, $95\%$CrI $= [`r round(quantile(rec.mutheta, probs = c(.025)), 2)`, `r round(quantile(rec.mutheta, probs = c(.975)), 2)`]$. This estimate can be used to inform the design of future studies.

# Accounting for Predictors

A key goal in meta-analysis is to assess the relationship between a predictor variable and the effect size of a dependent measure across studies [@Borenstein:etal:2011]. This goal is especially relevant when there is substantial variability of effect sizes. The hope then is that this variability can be explained by one or more predictors.  In the usual case, the researcher is concerned about the effect of the predictor on the meta-analytic mean, and fairly straightforward extensions of linear regression may be used to assess whether a predictor accounts for substantial variability.  The consideration of predictors, however, is far more nuanced in a does-every-study framework.

In meta-analysis, it is common to consider predictors that act on what we call *the study level*.  These predictors apply to the study as a whole. Examples could be elements of the experimental design, choice of dependent variable, the age of the target population, or the language of the participants.  In keeping with the theme of the development, models are specified for the case where researchers have summary-level data for each study and the values of the predictors for each study.

There are two main targets of inquiry: (a) is the inclusion of a predictor warranted, and (b) if so, does the inclusion change whether there are qualitative or quantitative differences in the set of studies.  The main approach here is to specify a set of models that encompass these targets and compare them with Bayes factors.

## Meta-regression Models

The meta-regression model for a single predictor is:

\begin{align} \label{eq:pred}
Z_i &\sim \mbox{Normal}(\theta_i, \frac{1}{N_i - 3}),\\
\theta_i &= \nu_i + x_i \beta,
\end{align}

where $x_i$ denotes the predictor for the $i$th study, and $\beta$ denotes the slope.  The parameter $\theta_i$ serves the same role in the previous models without a predictor.  To avoid identifiability issues, without any loss of generality we normalize the collection of $x_i$ such that the mean is zero and the variance is 1. Note that in this model, the parameter $\nu_i$ combines both a non-zero intercept and residuals.  It is not a classic intercept parameter inasmuch as there is one such parameter for each study.  It is not a classic residual as it is not zero-centered.  The reason for this specification is simple---the model reduces to the previous unconstrained model without predictors when the slope parameter, $\beta$ is zero.  Hence, it is a proper generalization of the previous development.  With the understanding that the collection of $\nu$s is not zero-centered, we will refer to them as the residuals.

Priors are needed on $\beta$ and the collection of $\nu_i$s. For $\beta$ we chose a weakly informative, zero-centered prior:
\[\beta \sim \mbox{Normal}(0, 0.15^2).\]
For $\nu_i$ we kept the prior structure previously shown in the Section "Prior Settings":

\begin{align*}
\nu_i &\sim \mbox{Normal}(\mu, \tau^2),\\
\mu &\sim \mbox{Normal}(0, 0.15^2),\\
\tau^2 &\sim \mbox{Inverse-Gamma}(1, 0.02).
\end{align*}


## Submodels and Interpretations

The critical question is on the placement of order constraints.  In the previous section, we discussed four different models on the true study effects---all were null, all were constant, all were positive, and a lack of constraint.  In the regression model, a comparable target is the parameter $\nu_i$, the residual effect *after* accounting for the predictor.  To indicate these models, we extend the previous notation as follows.  The model $\calM_{0+\beta}$ is a null model where the predictor is included and the residuals at the study level are all zero.  The models $\calM_{c+\beta}$, $\calM_{p+\beta}$, and $\calM_{u+\beta}$, the common residuals, positive residuals, and unconstrained residuals models, are defined likewise.  Note that some of these models are much more plausible than others. For example, a model where all residuals are identically zero seems unreasonable. Therefore, we carry the two models we consider most plausible and informative, $\calM_{p+\beta}$ and $\calM_{u+\beta}$.

These regression models are defined by constraint on the residuals.  Yet, we can also place constraint on the marginal or total effect, $\theta_i$.  And, surprisingly, the constraint on the marginals and on the residuals may have different implications.  The following cases highlight the differences and their theoretical interpretation:

i. *Explaining Qualitative Differences.* Consider the case where the total effect $\theta_i$ violates the positive constraints, that is, there is evidence that at least some $\theta_i$ are negative.  Here, we may conclude that the phenomenon is complex and disunified.  Now, suppose that when the predictor is added, the resulting residuals, $\nu_i$, obey positive constraints.  In this case, the rich results in the total is explained by the predictor.  That is, the predictor resulted in converting qualitative differences among the studies to quantitative ones.  This case, in our view, represents an advancement of knowledge---the predictor clearly explains why some studies are positive and others are negative.

ii. *Robust Quantitative Differences.*   One facet of inference with Bayes factor is that it is calibrated by the resolution of the data [@Rouder:etal:2009a].  When the data are noisy or few in number, simpler models are supported over more complicated ones.  In the context of this report, the null model is simplest, followed in order by the common model, the positive model, and the unconstrained model.  Adding a predictor that accounts for noise effectively increases the resolution of the data.  In some cases, if the predictor is added, it may account for variation, and the net result is that the data now have the resolution to support more complex models, say there is evidence for violations of constraint on the residuals that was previously unavailable.  Perhaps even more importantly, the same constraint that was obeyed in the total may still hold when predictors are added.  If there is still evidence that all studies show a true positive effect, then every-study-does holds even in light of increased resolution. The constraint is robust to the predictor.

We expand the notation as follows.  The model where there is constraint on residuals $\nu_i$ but not on totals $\theta_i$ are denoted as above.  The models where there is constraint on $\theta_i$ are indicated with parentheses as follows.  The model $\calM_{(p+\beta)_p}$ denotes the constraints where both $\theta_i$ and $\nu_i$ are positive.  To see if the finding is robust to the covariate, we may check that $\calM_{(p+\beta)_p}$ is superior to all other models. For example, to $\calM_{(p+\beta)_u}$, where with increased resolution from the covariate, we observe violations in the positivity of the marginals. Note that $\calM_{(p+\beta)_u}$ and $\calM_{p+\beta}$ are the same model as they have a constraint on the residuals but not the marginals.

## The Familiar-word-recognition Example

For the familiar-word-recognition meta-analysis, we assess the effect of the average age of study samples on the effect size. To do so, we standardize age and estimate the model using Gibbs sampling. Figure\ \ref{fig:resfig-cont}A comes from the previous analysis where there is no predictor. One of the things we can see here is shrinkage to the grand mean. Figure\ \ref{fig:resfig-cont}B now shows estimates for the meta-regression model with the predictor included. The gray open points again reference the observed effect sizes, and the blue points are the estimates for $\theta_i$ from the model including the predictor. Notice that the pattern of shrinkage is different: Observed effects are shrunken towards the regression line instead of the grand mean. The slope of the estimated regression line is $\beta = `r round(mean(beta_age), 2)`$, 95%CI $= [`r paste0(round(quantile(beta_age, probs = c(0.025, .975)), 2)[1], ", ", round(quantile(beta_age, probs = c(0.025, .975)), 2)[2])`]$. This slope is slightly reduced from the least square error regression line (dashed line). This reduction is due to hierarchical shrinkage of study effect sizes.

```{r resfig-cont, fig.cap= "Comparison of the meta-regression and the intercept-only analysis of age. The x-axes show the standardized average age per study, the y-axes show Fisher's Z effect size estimates. Gray open points are observed effect sizes; blue points are estimated effect sizes from the Bayesian models. A. Results for the intercept-only model. The dashed line corresponds to the average observed effect size; the blue line corresponds to the estimated mean from the Bayesian model. B. Results for the meta-regression model. The dashed line corresponds to a simple least squares regression line; the blue line corresponds to the estimated regression line from the Bayesian meta-regression model. The red dashed line and points denote $x_i\\beta$, the red vertical lines denote the non-zero-centered residuals $\\nu_i$.", fig.asp=0.5, warning=F}
layout(matrix(1:2, ncol = 2))
par(mgp = c(2, .7, 0), mar = c(3,3,2,1))

matplot(x = rbind(dat$mean_age_stand, dat$mean_age_stand), y = rbind(dat$z_calc, colMeans(rec.theta))
        , type = "l", lty = 1, col = "darkgray", bty = "n"
        , ylab = "Effect size", xlab = "Standardized Average Age"
        , ylim = c(-.2, .75))

points(dat$mean_age_stand, dat$z_calc, pch = 19, col = "white")
points(dat$mean_age_stand, dat$z_calc, col = "darkgray")
points(dat$mean_age_stand, colMeans(rec.theta), pch = 19, col = "slateblue")

abline(h = mean(dat$z_calc), lwd = 2, col = "gray30", lty = 2)
abline(h = mean(rec.mutheta), lwd = 2, col = "slateblue4")

legend("topleft"
       , legend = c("Observed", "Estimated")
       , pch = c(1, 19)
       , col = c("darkgray", "slateblue")
       , bty = "n")
mtext("A.", at = -4)

matplot(x = rbind(dat$mean_age_stand, dat$mean_age_stand), y = rbind(dat$z_calc, colMeans(pred))
        , type = "l", lty = 1, col = "darkgray", bty = "n"
        , ylab = "Effect size", xlab = "Standardized Average Age"
        , ylim = c(-.2, .75))

points(dat$mean_age_stand, dat$z_calc, pch = 19, col = "white")
matplot(x = rbind(dat$mean_age_stand, dat$mean_age_stand)
        , y = rbind(colMeans(pred), dat$mean_age_stand * mean(beta_age))
        , type = "l", lty = 1, col = adjustcolor("firebrick1", .5), bty = "n"
        , ylab = "Effect size", xlab = "Standardized Average Age", add = T)
points(dat$mean_age_stand, dat$z_calc, col = "darkgray")
points(dat$mean_age_stand, colMeans(pred), pch = 19, col = "slateblue")
points(dat$mean_age_stand, dat$mean_age_stand * mean(beta_age), pch = 19, col = "firebrick1")

abline(a = freq.rma.age$b[1,1], b = freq.rma.age$b[2,1], lwd = 2, col = "gray30", lty = 2)
abline(a = mean(mutheta_age)
, b = mean(beta_age), lwd = 2, col = "slateblue4")
mtext("B.", at = -4)

abline(a = 0
, b = mean(beta_age), lwd = 2, lty = 2, col = "firebrick1")

legend("topleft"
       , legend = c("Observed", "Estimated")
       , pch = c(1, 19)
       , col = c("darkgray", "slateblue")
       , bty = "n")
```

Figure \ref{fig:resfig-cont}B shows the effect of the predictor on the marginal estimates of the effect. But it does not indicate whether the predictor is evidenced. For that Bayes factors are needed. Table\ \@ref(tab:covariate-tab) shows Bayes factors for all models as compared to the unconstrained model. As a reminder, $\calM_{u+\beta}$ refers to the model with predictor with no ordinal constraints; $\calM_{p+\beta}$ refers to the model where all residuals, $\nu_i$, are constrained to be positive; and $\calM_{(p+\beta)p}$ refers to the model were in addition the marginal study effects, $\theta_i$ are constrained to be positive with the predictor taken into account. Figure \ref{fig:resfig-cont}B highlights the distinction between the latter two models: The dashed line is $x_i\beta$, the standardized regression line, and the red lines represent $\nu_i$ for each study. To compare $\calM_{u+\beta}$ and $\calM_{p+\beta}$, we assess if all red lines go upward from the dashed line, that is, if all $\nu_i > 0$. To compare $\calM_{p+\beta}$ and $\calM_{(p+\beta)p}$ we additionally assess if all blue points, the marginal study effects, are positive, that is, if all $\theta_i > 0$.

The Bayes factor analysis shows that the preferred model remains the positive-effects model without predictor. Model $\calM_{u+\beta}$, performs poorest with a Bayes factor of `r round(bfs.cont$bfs[3]/bfs.cont$bfs[4], 2)`-to-one in favor of the positive-effects model. Model $\calM_{(p+\beta)p}$, on the other hand, is the third-best model after the positive-effects and the common-effect models, with a Bayes factor of `r round(bfs.cont$bfs[1]/bfs.cont$bfs[4], 2)`-to-one in favor of the positive-effects model. We note that the best-performing model remains the positive-effects model. Hence, there is no evidence for the need to add the predictor.

```{r covariate-tab, results='asis'}
Model <- c("$\\calM_{u+\\beta}$", "$\\calM_{p+\\beta}$", "$\\calM_{(p+\\beta)p}$", "$\\calM_{u}*$"
           , "$\\calM_{p}$","$\\calM_{c}$", "$\\calM_{0}$")
BFs <- round(c((1/bfs.cont$bfs)[c(3:1)], 1, (1/bfs.cont$bfs)[c(4:6)]), 2)
tab <- cbind(Model, BFs)
colnames(tab) <- c("Model", "Bayes factor")
apa_table(tab
          , caption = "Bayes factors for the familiar-word-recognition meta-analysis."
          , note = "Bayes factors are in comparison to the unconstrained model."
          , row.names = FALSE
          , escape = F)
```

# Critical Predictors

As the replication crisis has roiled psychology, there have been a number of prominent failures to replicate previously established results [e.g. @OpenScienceCollaboration:2015; @Ebersole:etal:2016].  One popular set of beliefs is that the original results are in error, that is, the effects in question do not exist.  But another popular set of beliefs is that perhaps both sets of results are correct---that is, the original results correctly showed an effect and the replications correctly showed a lack of one.  The argument is that there must be some critical differences in the studies---critical hidden predictors that explain when the effect occurs and when it does not [@Monin:etal:2014].  The goal then is to find the missing critical predictors.

We are not fans of this approach, and were predisposed to think it is a waste of time.  Nonetheless, the does-every-study approach with predictors may be easily adapted to assess the conjecture that a critical predictor explains effects and their failures on a study-by-study basis.  In the following we develop the model for the conjecture.  Then, we apply it to the familiar-word-recognition example for a predictor that the original authors speculated explained the effect.  To our surprise, indeed, this predictor is critical and explains when the effect occurs and when it does not.  Unfortunately, the unavoidable conclusion is that the effect may merely be an artifact of publication bias. 

## The Critical Predictor Some-studies Model

For this approach we ask whether a categorical variable predicts whether a study has a positive effect, or, alternatively, no effect at all. For example, we may want to assess the effect of preregistration on effect size under the hypothesis that preregistered studies have no effect and unregistered studies have an effect. Alternatively, we might think that there is an effect for one experimental procedure but not for the other. We refer to this setup as a *some-studies* model, and in it the categorical variable is manifest.

```{r figModerator, fig.cap="Estimates for the covariate model with study type as some-studies covariate. The black points show estimated study effects from the model where some studies have a zero effect as predicted by the moderator (familiar-word-recognition effect as baseline) and some studies have positive effects as predicted by the moderator (familiar-word-recognition effect as main target of interest). For two studies (Bywater, 2004) the moderator was not reported.", fig.asp = 1, fig.width=7, warning=F}
## Second model

missingdat <- which(is.na(dat$main_or_baseline_study))
Is <- (1:I.all)[-missingdat]
Iord <- (1:I.all)[-which(order.ef %in% missingdat)]
pmmod <- colMeans(mod.theta) 
cis <- t(apply(mod.theta, 2, quantile, probs = c(.025, .975)))
order.mod <- rev(order(datmod$z_calc))

x.ind <- ifelse(datmod$main_or_baseline_study == "baseline", 1, 0)
ind.slab <- (1:length(pmmod))[x.ind == 1]
varest <- apply(mod.theta, 2, var)
weight_estimated <-1/varest
weight_estimated_scaled <- ((4 - 1) * (weight_estimated - min(weight_estimated[ind.slab]))) / (max(weight_estimated[ind.slab]) - min(weight_estimated[ind.slab])) + 2
weight_estimated_scaled <- ifelse(weight_estimated_scaled == Inf, 3, weight_estimated_scaled)

mat.ord <- cbind(cbind(pmmod, x.ind, cis, datmod$z_calc, weight_estimated_scaled)[order.mod,], line = Iord)
mat.ord.df <- as.data.frame(mat.ord)

dfBoth <- data.frame(effectSize = c(dat$z_calc[order.ef], mat.ord.df$pmmod),
                     y = c(1:I.all, mat.ord.df$line - .4),
                     weight_scaled = c(weight_scaled[order.ef], mat.ord.df$weight_estimated_scaled), 
                     lower = c(freqcipstudy[order.ef, 1], mat.ord.df$`2.5%`), 
                     upper = c(freqcipstudy[order.ef, 2], mat.ord.df$`97.5%`),
                     g = c(rep("Observed", I.all), rep("Estimated", length(Is))))
dfLab <- data.frame(studyLabels = dat$short_cite[order.ef]
                    , y <- 1:I.all)
    
plot <-  ggplot2::ggplot(dfBoth, ggplot2::aes(x = effectSize, y = y)) +
         ggplot2::geom_vline(xintercept = 0, linetype = "dotted") +
         ggplot2::geom_point(ggplot2::aes(shape = as.factor(dfBoth$g), colour = as.factor(dfBoth$g)), size = dfBoth$weight_scaled * .8) +
         ggplot2::geom_errorbarh(ggplot2::aes(xmin = dfBoth$lower, xmax = dfBoth$upper, colour = as.factor(dfBoth$g)), height = .1, show.legend = FALSE) +
         ggplot2::scale_y_continuous(breaks = dfLab$y, labels = as.character(dfLab$studyLabels)) +
         ggplot2::scale_color_manual("", values = c("black", "slategrey"), labels = c("Estimated", "Observed")) +
         ggplot2::scale_shape_manual("", values = c(16, 15)) +
         ggplot2::guides(shape = ggplot2::guide_legend(reverse=TRUE, override.aes = list(size=3)), colour = ggplot2::guide_legend(reverse=TRUE)) +
         ggplot2::theme(axis.text.y.right = ggplot2::element_text(colour = c(rep(c("slategrey", "black"), each = I.all)))) +
         ggplot2::xlab(expression("Z and "*theta)) +
         ggplot2::ylab(" ")
plot
```

With the base model from Equation\ \@ref(eq:basemodel) the some-studies model can be placed on $\theta_i$:

\[
\begin{array}{llr}
\calM_{ss}: && \begin{cases}
\theta_i \stackrel{iid}{\sim} \mbox{Normal}^+(\mu,\tau^2) \quad \mbox{if} \; x_i = 1,\\
\theta_i = 0 \quad \quad \quad \quad \quad \quad \quad \; \mbox{if} \; x_i = 0.
\end{cases}
\end{array}
\]

where $x_i$ is an indicator variable for the dichotomous predictor at hand. Note that this model is quite constrained: Not only does it predict the direction of the effect for some studies, it also predicts zero-effects for other studies, and it *a priori* assigns each study to either of the two categories.

In the meta-analysis of familiar-word-recognition we consider a predictor called *study use*. Study use refers to whether the familiar-word-recognition effect was used as the target of study or as a baseline when exploring another effect. Through correspondence the original authors [@Carbajal:etal:2020] hypothesized that, because data collection in developmental psychology is costly, the data were published irrespective of the outcome if the familiar-word-recognition effect was the target of interest. Conversely, if the effect was considered a baseline it would perhaps be omitted from publication if the outcome was a null finding. As a result, the familiar-word-recognition effect may be inflated for studies where it was considered a baseline effect. We test Carbajal et al.'s conjecture here.

Figure\ \@ref(fig:figModerator) shows effect size estimates from the model for `r length(Is)` of the `r I.all` studies where it was clear whether the familiar-word-recognition effect was either target or baseline. In the majority of the studies with small effects the familiar-word-recognition effect was the main target of interest; in the majority of the studies with larger effects the familiar-word-recognition effect was the baseline.  We compared the some-studies model with the models from the previous sections using Bayes factors (See Table\ \@ref(tab:ss-tab)).^[The Bayes factors for the models shown both in Table 1 and Table\ \@ref(tab:ss-tab) deviate slightly because less studies are included for Table\ \@ref(tab:ss-tab).] In accordance with Carbajal et al.'s conjecture, the some-studies model outperforms all the other models, and it is preferred to the positive-effects model by about `r round(bfs.ss$bfs[2]/bfs.ss$bfs[1])` to 1. 

```{r ss-tab, results = 'asis'}
Model <- c("$\\calM_{u}*$", "$\\calM_{ss}$", "$\\calM_{p}$","$\\calM_{c}$", "$\\calM_{0}$")
BFs <- round(c(1, (1/bfs.ss$bfs)), 2)
tab <- cbind(Model, BFs)
colnames(tab) <- c("Model", "Bayes factor")
apa_table(tab
          , caption = "Bayes factors for the target vs. baseline study analysis."
          , note = "Bayes factors are in comparison to the unconstrained model."
          , row.names = FALSE
          , escape = F)
```

# Software

The analyses presented here are conducted in `R` [@R-base].^[The following packages were used: `r cite_r("r-references.bib")`.] As we hope this development is attractive to many researchers we took the following steps to make the adoption convenient. First, we provide an `R`-script containing all necessary functions to compute posterior estimates and Bayes factors at [github](https://github.com/jstbcs/meta-poor/blob/public/share/lib.R). Second, we developed the Gibbs sampling functions that can be used on new data.^[Some of the analyses provided here are also supported by the `R`-package `metaBMA` [@Heck:etal:2019].] The posterior distribution for models without predictors are estimated using the function `mcmc_unconstrained()`;  the posterior distribution for models with predictors are estimated using the function `mcmc_predictor()`; and the posterior distribution for models without critical predictors are estimated using the function `mcmc_somesstudies()`. Functions `get.bfs.ss()` and `get.bfs.cont()` provide Bayes factor estimation for all proposed models. The analysis of the familiar-word-recognition effect [@Carbajal:etal:2020] is provided at [github](https://github.com/jstbcs/meta-poor/blob/public/papers/submission/analysis/analysis_carbajal.Rmd) as well. 

# Prior Sensitivity

Bayes factor model comparison requires the choice of settings on prior distributions. Some prior settings have minimal influence on the results of the analysis while others have substantial influence. In the current analysis the settings of $c_1$ and $c_2$ have the largest influence. One way of assessing the effect of the prior choices on inference is to conduct a sensitivity analyses. In a sensitivity analysis, prior settings are adjusted within reasonable ranges and the resulting variability of Bayes factors is assessed [e.g., @Haaf:Rouder:2017; @Heycke:etal:2018]. Here, we conduct a sensitivity analysis for the familiar-word-recognition effect. We chose to double and halve the scale on $\mu$, $c_1$, and the scale on $\tau$, $c_2$ from our original setting (between 0.08 and 0.30).

```{r tabsens, results='asis'}
tabsens <- rbind(set0, set1, set2, set3, set4)
getbfpretty <- function(bfs){
  c(1/bfs[1], bfs[2]/bfs[1], bfs[3]/bfs[1], bfs[4]/bfs[1])
}
bfsens <- round(rbind(getbfpretty(bfs.ss$bfs)
                      , getbfpretty(bfs.sens1$bfs)
                      , getbfpretty(bfs.sens2$bfs)
                      , getbfpretty(bfs.sens3$bfs)
                      , getbfpretty(bfs.sens4$bfs)), 2)

tabsens <- cbind(tabsens[, -1], bfsens)
colnames(tabsens) <- c("Setting $c_1$", "Setting $c_2$"
                       , "$BF_{ssu}$", "$BF_{ssp}$", "$BF_{ssc}$", "$BF_{ss0}$")

apa_table(tabsens, caption = "Prior sensitivity analysis.", row.names = F, escape = F)
```

Table\ \@ref(tab:tabsens) shows the Bayes factors for the original analysis (top row) and four additional analyses with varied settings. Bayes factors are between the preferred some-studies model and the unconstrained, positive-effects, common-effect, and null models, respectively. If all values in a row remain above 1 it implies that the some-studies model is superior for the particular prior setting. However, if a values is below 1 it implies that an alternative model is superior, and therefore the inference is not robust to the chosen prior settings. For the data at hand, while Bayes factors vary considerably, the some-studies model is preferred for all settings in the sensitivity analysis. The pattern of Bayes factors is fairly straight-forward: For smaller scale settings the Bayes factor between the positive-effects model and the some-studies model is relatively close to 1; for larger scales the null model is actually preferred over the positive-effects model, and this phenomenon is well-understood as Lindley's paradox [@Lindley:1957]. Yet, within these reasonable ranges of prior settings, the some-studies model is robustly preferred over all of its alternatives. 

# Discussion

Meta-analysis is a powerful tool to summarize evidence across several studies. In the absence of the raw data it is the most straight-forward method to combine quantitative results across multiple studies. Here, we address the fundamental issue in meta-analysis of what forms an appropriate summary across studies. Rather than focusing on an overall effect size, we ask whether every study in the data set plausibly shows an effect in the expected direction. The current approach is motivated by the presence of coarse theory in psychology and the desire for robustness and replicability across many design elements. When theory is coarse, it provides at most an ordinal prediction on effects [@Haaf:etal:2019]. If all studies plausibly show a true effect in the same direction, then the phenomenon may robustly be found across varying operationalizations and study settings. To answer the question whether every study shows a true effect in the same direction we develop a set of models: A general model much like the conventional meta-analytic random-effects model; a positive-effects model where all studies have a true effect in the same direction; a common-effect model where all true effects are the same; and a null model where all true effects are identically zero. To assess the relative strength of evidence between these models, we propose a Bayes factor model comparison approach adapted from @Haaf:Rouder:2017 and @Rouder:etal:2019b. The crucial innovation here is that the new approach takes summary statistics from each study instead of raw data.

A second scientific question for meta-analysis concerns the role of predictor variables. We develop a new set of models that describe how covariates affect the direction of effects. We apply the does-every-study meta-analysis to the familiar-word-recognition effect [@Carbajal:etal:2020] in babies. In developmental research, small sample sizes are common, and meta-analysis therefore remains a central methodological approach. The results from our analysis provide evidence for the claim that studies either show a true positive effect, or truly no effect whatsoever. We did not find a qualitative interaction where some studies truly show an opposite effect. Importantly, there seems to be a critical predictor that determines whether a study will show an effect or not. Studies designed to test the effect result in null effects. Those that use the effect incidentally show positive effects. We suspect publication bias when the effect is not the target of the study.

## Limitations and Future Directions

With every model development come model assumptions that need to be taken into account. The advantage of the current approach is that it only requires surface summary statistics. However, modeling these surface statistics introduces the assumption that the observed within-study variance is the true variance. This assumption is actually common in many linear modeling setups such as the computation of BIC. How good of an assumption it is is simply a function of the size of the sample. With large sample sizes obviously the observed variance is very close to the true one. With smaller studies such as in developmental psychology, however, the true variance might be quite different.

Another limitation of the current approach is that, except in some cases, it does take publication bias into account. Although we use the some-studies approach to identify a critical predictor we speculate that publication bias plays a role without actually modeling it. Yet, publication bias and questionable research practices are intimately tied to the question of the utility of meta-analysis [@Carter:etal:2017; @Corker:2018]. If the studies in the meta-analytic set are heavily biased, using them to learn about the population is difficult.

One of the main innovations in meta-analysis is the proposal of models and methods to correct for publication bias  [@Duval:Tweedie:2000; @Simonsohn:etal:2014; @Stanley:Doucouliagos:2014; @Hedges:1984; @Iyengar:Greenhouse:1988]. One possible extension of the current modeling approach is to model selection effects simultaneously with true effects [@Iyengar:Greenhouse:1988; @Maier:etal:2020]. In principle, this extension should not be too difficult. Yet, assessing the success of these corrections can be problematic as neither the process of study censorship nor the amount of publication bias are known in any real meta-analytic set [@Guan:Vandekerckhove:2016]. One additional difficulty here is the role of skewness of the distribution of observed effects. In the current approach, skewness comes from an underlying skewed distribution of true effects; in most publication bias correction models, in contrast, skewness reflects the censoring process. While it may be possible to disentangle publication bias and true positive effects by assessing the relationship of sample size and effect size, a robust, general solution remains an open issue.

A third limitation of the current approach concerns the type of predictors considered in the meta-regression analysis. Here, we only developed a framework for study-level predictors. However, it is increasingly common to assess the effect of within-study predictors such as age per study condition, or different dependent variables assessed within the same sample. If such within-study predictors are included, a three-level meta-analysis has to be conducted to account for the within-study dependency [@VandenNoortgate:etal:2013; @Cheung:2014]. One future improvement may be the extension of every-study-does meta-analysis to three-level meta-analysis.

## Conclusion

<!-- The main contribution of the current analysis compared to the analysis proposed by @Rouder:etal:2019b is modeling surface statistics instead of raw data. This approach is necessary when raw study data are not available. Whenever raw data are available, we recommend using hierarchical model approaches instead of surface statistics models. Hierarchical models are better equipped to optimally model within-study variability. As open science gains more and more traction, however, it may be possible that raw data are available for some newer studies but not others. To optimally incorporate information from raw data and surface data, a hybrid modeling approach integrating the models proposed by @Rouder:etal:2019b and our current analysis might be useful. -->

In summary, the current approach allows to investigate questions of ordinal constraint in a meta-analytic setting. We think that answering the does-every-study question is timely and topical. If, indeed, every study shows a true effect in the same, expected direction, maintaining an underlying theory for the target phenomenon seems reasonable. If, however, the ordinal constraint is violated and some studies show true opposite effects, researchers need to investigate the underlying mechanism that lead to this qualitative interaction. These mechanisms may be found in design or population, and the proposed predictor analysis can help understanding the underlying differences.

\newpage

# References
```{r create_r-references}
# r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup

\newpage
# Appendix

The asymptotic distribution of $Z$ is well established for the correlation between two independent and identically distributed normal random variables [@Ferguson:1996]. In the case of correlation, Fisher's $Z$ serves as normalized correlation measure. Yet, how does Fisher's $Z$ translate to experimental data? Let's assume an experimental setup with $j$ conditions, $j = 1, 2$. The $i$th participant is assigned to one of the two conditions, $i = 1, \ldots, 2n$, where $n$ is the number of participants in each condition and $2n$ is the total number of participants. Then $Y_i$ denotes the $i$th person's observation with 

\begin{align}\label{eq:defY}
Y_{i} \sim \begin{cases} \mbox{Normal}(\mu_1, \sigma^2) & \mbox{if } i = 1, \ldots, n,\\
\mbox{Normal}(\mu_2, \sigma^2) & \mbox{if } i = n + 1, \ldots, 2n,\end{cases}
\end{align}

where $\mu_j$ denotes the true mean of the $j$th condition and $\sigma^2$ denotes the within-group variance. To calculate Fisher's $Z$ as effect size measure we could first calculate Cohen's $d$ and then use common transformations via the correlation coefficient $r$. To understand what this correlation coefficient represents we may define a new variable, $X_i$, that denotes $i$th participant's condition. $X_i$ is an effect variable encoding $j = 1$ as $X_i = -1$ and $j = 2$ as $X_i = 1$. The correlation coefficient $r$ is the point-biserial correlation between $\mathbf{X}$ and $\mathbf{Y}$, and Fisher's $Z$ may be calculated using Equation\ \@ref(eq:Zcalc). Yet, we need to verify that the asymptotic distribution of Fisher's $Z$ for the biserial correlation is the same as the asymptotic distribution of Fisher's $Z$ for the bivariate normal. Here, we provide a proof that the asymptotic variance is stable and does not depend on the size of the effect. 

Let $\bar{Y}$ be the grand mean of $\mathbf{Y}$, $\bar{Y} = 1/2 (\bar{Y}_1 + \bar{Y}_2)$. Here, $\bar{Y}_1$ denotes the mean of $Y_i$ for $i = 1, \ldots, n$, and $\bar{Y}_2$ denotes the mean of $Y_i$ for $i = n + 1, \ldots, 2n$. Let $S_{YY}$ denote the variance of $\mathbf{Y}$, $S_{YY} = \sum_1^n (Y_i - \bar{Y}_1)^2 + \sum_{n+1}^{2n} ( Y_i - \bar{Y}_2)^2 + n/2 (\bar{Y}_1 - \bar{Y}_2)^2 = SS_1 + SS_2 + n/2 (\bar{Y}_1 - \bar{Y}_2)^2$, where $SS_1$ is the sum of squares for $i = 1, \ldots, n$, and $SS_2$ is the sum of squares for $i = n + 1, \ldots, 2n$. Let $S_{XX} = 2n$ denote the variance of $\mathbf{X}$ and let $S_{XY} = n(\bar{Y}_1 - \bar{Y}_2)$ denote the covariance of $\mathbf{X}$ and $\mathbf{Y}$. We can now insert these quantities into the formula for the correlation coefficient $r$:

\begin{align*}
r &= \frac{S_{XY}}{\sqrt{S_{XX}S_{YY}}}\\
&= \frac{\sqrt{1/2}(\bar{Y}_1 - \bar{Y}_2)}{\sqrt{\frac{2(SS_1 + SS_2)}{2n} + 1/2(\bar{Y}_1 - \bar{Y}_2)^2)}}.
\end{align*}

We may define a new random variable $m_n = \bar{Y}_1 - \bar{Y}_2$. Then, based on the Central Limit Theorem:

\[\sqrt{n}(m_n - \Delta) \stackrel{\mathscr{D}}{\rightarrow} \mbox{Normal}(0, 2 \sigma^2),\]

where $\Delta$ is the true effect. The pooled variance of $\mathbf{Y}$ is $S_n = \frac{SS_1 + SS_2}{2n}$.^[Note that $S_n$ is not the variance of $\mathbf{Y}$, but the pooled variance of $Y_i$ if $i = 1, \ldots, n$ and $Y_i$ if $i = n + 1, \ldots, 2n$] The Central Limit Theorem gives $\sqrt{n}(S_n - 2\sigma^2) \stackrel{\mathscr{D}}{\rightarrow} \mbox{Normal}(0, 4\sigma^2)$. Using the two asymptotic distributions, $r \rightarrow \frac{\sqrt{1/2}(\Delta)}{\sqrt{4\sigma^2 + \Delta^2/2}}$. Let $\Sigma$ be the variance-covariance matrix of $\big(\begin{smallmatrix} m_n\\ S_n \end{smallmatrix}\big)$:

\[\Sigma = 
\begin{bmatrix}
2\sigma^2 & 0\\
0 & 4\sigma^2
\end{bmatrix}.
\]

And let $h(a, b) = \frac{\sqrt{1/2}(a)}{\sqrt{2b + a^2/2}}$. The partial derivatives of $h(a, b)$ are:

\begin{align*}
\frac{\partial h}{\partial a} &= \frac{\sqrt{2} b}{(2b + a^2/2)^{3/2}},\\
\frac{\partial h}{\partial b} &= -\frac{a}{\sqrt{2}(2b + a^2/2)^{3/2}}.
\end{align*}

Using the Delta-method [@Ferguson:1996], we may express the asymptotic distribution of $h(m_n, S_n)$ as

\[\sqrt{n}(h(m_n, S_n) - h(\Delta, 2\sigma^2) \stackrel{\mathscr{D}}{\rightarrow} \mbox{Normal}(0, \Sigma^*),\]

where $h(m_n, S_n)$ is the correlation coefficient, the mean, $h(\Delta, 2\sigma^2)$, is the true correlation coefficient, $\rho$, and the asymptotic variance is 

\begin{align*}
\Sigma^* &= \Big(\frac{2\sqrt{2} \sigma^2}{(4\sigma^2 + \Delta^2/2)^{3/2}}\Big)^2 2\sigma^2 + \Big(\frac{- \Delta}{\sqrt{2}(4\sigma^2 + \Delta^2/2)^{3/2}}\Big)^2 4\sigma^2 \\
&= \frac{16}{\big(8 + \frac{\Delta^2}{\sigma^2}\big)^2}.
\end{align*}

We may apply Fisher's $Z$ transformation to stabilize the variance. Let $g(r) = 1/2 \log \big(\frac{1 + r}{1 - r}\big)$. Then $g'(r) = \frac{1}{1 - r^2}$. Substituting $h(\Delta, 2\sigma^2)$ for $\rho$, $g'(\rho) = \frac{8 + \frac{\Delta^2}{\sigma^2}}{8}$. We may again use the Delta-method to express the asymptotic distribution of $Z$:

\[\sqrt{n}(g(r) - g(\rho) \stackrel{\mathscr{D}}{\rightarrow} \mbox{Normal}(0, (g'(\rho))^2 \Sigma^*),\]

where $g(r) = Z$ and $g(\rho) = \theta$. The asymptotic variance is

\[
(g'(\rho))^2 \Sigma^* = \frac{\cancel{(8 + \frac{\Delta^2}{\sigma^2})^2}}{8^2} \frac{16}{\big(\cancel{8 + \frac{\Delta^2}{\sigma^2}\big)^2}} = 1/4.
\]

Crucially, this value, $1/4$, does not depend on $\rho$.

